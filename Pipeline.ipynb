{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "import os\n",
    "import sys\n",
    "from datetime import date\n",
    "current_date = date.today()\n",
    "import pyodbc\n",
    "import getpass\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=172.18.102.72;DATABASE=Epic;UID=partners\\md1010;PWD=getpass.getpass();Trusted_Connection=yes')\n",
    "%load_ext autotime\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2, venn3\n",
    "import warnings\n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools\n",
    "from more_itertools import unique_everseen\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandasql as ps\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"\"\"select top 1000 * from epic....... \"\"\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If required fetch data in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 5\n",
    "offset = 0\n",
    "dfs = []\n",
    "while True:\n",
    "  sql = \"select * from.. order by .. OFFSET %d ROWS FETCH NEXT %d ROWS ONLY\" % (offset,chunk_size) \n",
    "  dfs.append(pd.read_sql(sql, conn))\n",
    "  offset += chunk_size\n",
    "  if len(dfs[-1]) < chunk_size:\n",
    "    break\n",
    "full_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'column': 'column', 'column': 'column'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FixDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes only the date from the date time stamp 2020-01-01 12.00pm\n",
    "df['ContactDTS'] = df['ContactDTS'].str.split(' ').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ContactDTS\"] = pd.to_datetime(df[\"ContactDTS\"]).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values([\"column\", \"column\"], ascending = (True,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# taking only non nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['column'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.merge(test1, new_df, left_on='column', right_on='column', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a flag on if one column exist in another dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flag'] = np.where(df.column.isin(df.column), 'Y', 'N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop if one column exist in another dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = df1['column_name'].isin(df2['column_name'])\n",
    "df1.drop(df1[cond].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string conversion\n",
    "df['NoteTXT'] = df['NoteTXT'].astype(str)\n",
    "#join notes with same csnid\n",
    "df['NoteTXT'] = df.groupby(['NoteCSNID'])['NoteTXT'].transform(lambda x : ' '.join(x))\n",
    "#remove duplicate csnid\n",
    "df = df.drop_duplicates(subset='NoteCSNID', keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take max of csnid for noteid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = RPDR_BLC_2_A_B_EDW_Notes.groupby(['NoteID'])['NoteCSNID'].transform(max) == RPDR_BLC_2_A_B_EDW_Notes['NoteCSNID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPDR_BLC_2_A_B_EDW_Notes = RPDR_BLC_2_A_B_EDW_Notes[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# condition_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditions(s):\n",
    "    if (s['icd9cd_match_flag'] == 1) or (s['icd10cd_match_flag'] == 1) or (s['diagnosis_match_flag'] == 1):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_diagnosis['icd_flag'] = patient_diagnosis.apply(conditions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an export to folder with current date\n",
    "newpath = r'C:\\Users\\deodh\\Dropbox (Partners HealthCare)\\NLP\\Annotator_Data\\EDW_data\\EDW_data_'+str(current_date)\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "output_file = os.path.join(newpath,'df.csv')\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export large excel using zip64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter (r\"C:\\Users\\MIND_DS\\Dropbox (Partners HealthCare)\\BCG\\Mayuresh\\BCG\\data\\source_data\\RPDR_BLC_2_A\\SD587_20210421_135453_Lno.xlsx\", engine = 'xlsxwriter') as writer:\n",
    "    writer.book.use_zip64 ()\n",
    "    lno_notes.to_excel (writer,index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export excel in batches by number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROW_LIMIT = 75000\n",
    "chunks = (len(bcg_regex_match_dataset) // ROW_LIMIT) + 1\n",
    "for i, chunk in enumerate(np.array_split(bcg_regex_match_dataset, chunks)):\n",
    "    chunk.to_excel(f'bcg_regex_match_dataset{i+1}.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export csvs in batches by number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size = 75000\n",
    "batch_no = 1 \n",
    "\n",
    "for chunk in pd.read_csv('lno_notes_blc_regex_match_dataset.csv', chunksize = chunk_size):\n",
    "    chunk.to_csv('lno_notes_blc_regex_match_dataset' + str(batch_no) + '.csv',index = False)\n",
    "    batch_no +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# before applying regex context code add these 100 '-' before and after the note other wise regex match found at the start of the note gives empty context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lno_notes[\"Comments\\n\"] = sample_lno_notes[\"Comments\\n\"].str.replace('\\s+', ' ', regex=True) # remove uneccessary white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lno_notes['Comments\\n'] = ' ------------------------------------------------------------------------------------------------------------- ' + lno_notes['Comments\\n'].astype(str)+' ------------------------------------------------------------------------------------------------------------- '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code for finding context from the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l = []\n",
    "for r in tqdm(lno_notes['Comments\\n']):\n",
    "    try:\n",
    "        l.append([m.span() \\\n",
    "                  for m in re.finditer(re.compile(\"|\".join(regex[\"REGEX\"]), \n",
    "                                                  re.IGNORECASE),r)])\n",
    "    except:\n",
    "        l.append([])\n",
    "        \n",
    "        \n",
    "lno_notes[\"regex_location\"] = l\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "l = []\n",
    "for i in tqdm(range(len(lno_notes[\"regex_location\"]))):\n",
    "    s = []\n",
    "    try:\n",
    "        for j in lno_notes[\"regex_location\"][i]:\n",
    "            s.append(lno_notes[\"Comments\\n\"][i][j[0]-150 : j[0]+150])\n",
    "    except:\n",
    "        pass\n",
    "    l.append(s)\n",
    "    \n",
    "    \n",
    "lno_notes[\"regex_sent\"] = l\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to create new row for each regex context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes['regex_sent'] = notes['regex_sent'].str.replace('\"', \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = (notes.assign(regex_sent = notes['regex_sent'].str.split(\"', '\")).explode('regex_sent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replace [] with nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes['regex_sent'] = notes['regex_sent'].astype(str)\n",
    "notes['regex_sent'] = notes['regex_sent'].str.replace('[', '')\n",
    "notes['regex_sent'] = notes['regex_sent'].str.replace(\"]\", '')\n",
    "notes = notes.replace(r'^\\s*$', np.NaN, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = notes[notes['regex_sent'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code for finding regex match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for note in tqdm(notes[\"regex_sent\"]):\n",
    "    try:\n",
    "        l.append(re.findall(re.compile(\"|\".join(regex[\"REGEX\"]), re.IGNORECASE), note))\n",
    "    except:\n",
    "        l.append([])\n",
    "\n",
    "notes[\"regex_match\"] = l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replace [] with nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes['regex_match'] = notes['regex_match'].astype(str)\n",
    "notes['regex_match'] = notes['regex_match'].str.replace('[', '')\n",
    "notes['regex_match'] = notes['regex_match'].str.replace(\"]\", '')\n",
    "notes = notes.replace(r'^\\s*$', np.NaN, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = notes[notes['regex_match'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a flag wherever notnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcg_regex_match_dataset['regex_flag'] = bcg_regex_match_dataset['regex_flag'].notnull()*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply json format to the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['regex_match'] = test['regex_match'].apply(json.dumps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
